---
title: Utilities
description: A list of various utilities provided by TypeGPU.
---

## *prepareDispatch*

The `prepareDispatch` function simplifies running simple computations on the GPU.
Under the hood, it wraps the callback in a `TgpuFn`, creates a compute pipeline, and returns a dispatch function to execute it.

This can help reduce serialization overhead when initializing buffers with data.

```ts twoslash
import tgpu, { prepareDispatch } from 'typegpu';
import * as d from 'typegpu/data';

const root = await tgpu.init();

const Boid = d.struct({
  index: d.u32,
  pos: d.vec3f,
});

// buffer of 2048 Boids
const boidsMutable = root.createMutable(d.arrayOf(Boid, 2048));

const dispatch = prepareDispatch(root, (x) => {
  'kernel';
  const boidData = Boid({ index: x, pos: d.vec3f() });
  boidsMutable.$[x] = boidData;
});
// run callback for each x in range 0..2047
dispatch(2048);
```

:::note
Remember to mark the callback with `'kernel'` directive to let TypeGPU know that this function is TGSL.
:::

The returned dispatch function can be called multiple times.
Since the pipeline is reused, there’s no additional overhead for subsequent calls.

```ts twoslash
import tgpu, { prepareDispatch } from 'typegpu';
import * as d from 'typegpu/data';
const root = await tgpu.init();
// ---cut---
const data = root.createMutable(d.arrayOf(d.u32, 8), [0, 1, 2, 3, 4, 5, 6, 7]);

const doubleUp = prepareDispatch(root, (x) => {
  'kernel';
  data.$[x] *= 2;
});

doubleUp(8);
doubleUp(8);
doubleUp(4);

// no need to call `onSubmittedWorkDone()` because the command encoder
// will queue the read after `doubleUp` anyway
console.log(await data.read()); // [0, 8, 16, 24, 16, 20, 24, 28]
```

The callback can have up to three arguments (dimensions).
Buffer initialization commonly uses random number generators.
For that, you can use the [`@typegpu/noise`](TypeGPU/ecosystem/typegpu-noise) library.

```ts twoslash
import tgpu, { prepareDispatch } from 'typegpu';
import * as d from 'typegpu/data';
// ---cut---
import { randf } from '@typegpu/noise';

const root = await tgpu.init();

// buffer of 1024x512 floats
const waterLevelMutable = root.createMutable(
  d.arrayOf(d.arrayOf(d.f32, 512), 1024),
);

prepareDispatch(root, (x, y) => {
  'kernel';
  randf.seed2(d.vec2f(x, y).div(1024));
  waterLevelMutable.$[x][y] = 10 + randf.sample();
})(1024, 512);
// callback will be called for x in range 0..1023 and y in range 0..511

// (optional) read values in JS
console.log(await waterLevelMutable.read());
```

It is highly recommended NOT to use `dispatch` for:

- More complex compute shaders.
When using `dispatch`, it is impossible to switch bind groups or to change workgroup sizes.
For such cases, a manually created pipeline would be more suitable.

- Small calls.
Usually, for small data the shader creation and dispatch is more costly than serialization.
Small buffers can be more efficiently initialized with `buffer.write()` method.

:::note
The default workgroup sizes are:

- `[1, 1, 1]` for 0D dispatches,
- `[256, 1, 1]` for 1D dispatches,
- `[16, 16, 1]` for 2D dispatches,
- `[8, 8, 4]` for 3D dispatches.

The callback is not called if the global invocation id of a thread would exceed the size in any dimension.
:::

## *batch*
By default, TypeGPU pipelines and render passes are submitted to the GPU immediately.
If you want to give the GPU an opportunity to better utilize its resources,
you can use the `batch` function.

The `batch` function allows you to submit multiple pipelines and render passes to the GPU in a single call.
Under the hood, it creates `GPUCommandEncoder`,
records the commands from the provided callback function,
and submits the resulting `GPUCommandBuffer` to the device.

:::caution
Read–write operations always flush the command encoder.
When you call a pipeline with a performance callback, the callback is invoked at the end of the batch. The timestamps themselves are not affected by the batching. They are still written at the beginning and/or end of the associated pipeline/render pass.
We've prepared a table showing when a flush occurs (i.e., when a new command encoder is created). Keep this in mind when using `batch`.
:::

| Invocation                        | Inside batch env   | Outside batch env  |
|-----------------------------------|--------------------|--------------------|
| raw pipeline                      | No Flush ❌        | Flush ✅           |
| pipeline with performance callback| Flush ❌ / ⚠️        | Flush ✅           |
| pipeline with timestamp writes    | No Flush ❌        | Flush ✅           |
| beginRenderPass                   | No Flush ❌        | Flush ✅           |
| write                             | Flush ✅           | Flush ✅           |
| read                              | Flush ✅           | Flush ✅           |

```ts twoslash
import tgpu from 'typegpu';
import * as d from 'typegpu/data';

const entryFn = tgpu['~unstable'].computeFn({ workgroupSize: [7] })(() => {});
const vertexFn = tgpu['~unstable'].vertexFn({
  out: { pos: d.builtin.position },
})(() => {
  return { pos: d.vec4f() };
});
const fragmentFn = tgpu['~unstable'].fragmentFn({
  out: d.vec4f,
})(() => d.vec4f());

const root = await tgpu.init();

const renderPipeline = root['~unstable']
  .withVertex(vertexFn, {})
  .withFragment(fragmentFn, { format: 'rgba8unorm' })
  .createPipeline();

const computePipeline = root['~unstable']
  .withCompute(entryFn)
  .createPipeline();

const buffer = root.createBuffer(d.arrayOf(d.f32, 1024));

// ---cut---
const render = () => {
  computePipeline.dispatchWorkgroups(7, 7, 7);
  renderPipeline.draw(777);
  // more operations...

  buffer.write(Array.from({ length: 1024 }, () => Math.random()));
  // force flush caused by write, new command encoder
};

root['~unstable'].batch(render);
```

:::note
The calls in the batch callback have to be made synchronously.
This may seem limiting, but our reasoning is that if you need to wait for something inside
a batch, just split the batch in two.
:::

:::danger
Nested batching is not supported and will result in a runtime error.
:::

## *console.log*

Yes, you read that correctly, TypeGPU implements logging to the console on the GPU!
Just call `console.log` like you would in plain JavaScript, and open the console to see the results.

```ts twoslash
import tgpu, { prepareDispatch } from 'typegpu';
import * as d from 'typegpu/data';

const root = await tgpu.init();
// ---cut---
const callCountMutable = root.createMutable(d.u32, 0);
const dispatch = prepareDispatch(root, () => {
  'kernel';
  callCountMutable.$ += 1;
  console.log('Call number', callCountMutable.$);
});

dispatch();
dispatch();

// Eventually...
// "[GPU] Call number 1"
// "[GPU] Call number 2"
```

Under the hood, TypeGPU translates `console.log` to a series of serializing functions that write the logged arguments to a buffer that is read and deserialized after every draw/dispatch call.

The buffer is of fixed size, which may limit the total amount of information that can be logged; if the buffer overflows, additional logs are dropped.
If that's an issue, you may specify the size manually when creating the `root` object.

```ts twoslash
import tgpu, { prepareDispatch } from 'typegpu';
import * as d from 'typegpu/data';

const presentationFormat = undefined as any;
const canvas = undefined as any;
const context = canvas.getContext('webgpu') as any;
// ---cut---
const root = await tgpu.init({
  unstable_logOptions: {
    logCountLimit: 32,
    logSizeLimit: 8, // in bytes, enough to fit 2*u32
  },
});

/* vertex shader */

const mainFragment = tgpu['~unstable'].fragmentFn({
  in: { pos: d.builtin.position },
  out: d.vec4f,
})(({ pos }) => {
  // this log fits in 8 bytes
  // static strings do not count towards the serialized log size
  console.log('X:', d.u32(pos.x), 'Y:', d.u32(pos.y));
  return d.vec4f(0, 1, 1, 1);
});

/* pipeline creation and draw call */
```

:::note
The logs are written to console only after the dispatch finishes and the buffer is read.
This may happen with a noticeable delay.
:::

:::caution
When using `console.log`, atomic operations are injected into the WGSL code to safely synchronize logging from multiple threads.
This synchronization can introduce overhead and significantly impact shader performance.
:::

There are some limitations (some of which we intend to alleviate in the future):

- `console.log` only works when used in TGSL, when calling or resolving a TypeGPU pipeline.
Otherwise, for example when using `tgpu.resolve` on a WGSL template, logs are ignored.
- `console.log` only works in fragment and compute shaders.
This is due to [WebGPU limitation](https://www.w3.org/TR/WGSL/#address-space) that does not allow modifying buffers during the vertex shader stage.
- TypeGPU needs to handle every logged data type individually.
Currently, the only supported types are `bool`, `u32`, `vec2u`, `vec3u` and `vec4u`.
- `console.log` currently does not support template literals and string substitutions.
- Other `console` methods like `clear` or `warn` are not yet supported.
